<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Pytorch-Tensor基础 | Code Play</title><meta name="author" content="南懿烨曦,869083577@qq.com"><meta name="copyright" content="南懿烨曦"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="tensor中的shape和size()反映了tensor的格式，二维的tensor的shape&#x2F;size()对应矩阵中的行和列的大小，这样理解即可 1234567891011121314151617181920212223242526272829303132333435import torch# 创建0维的tensor-&gt;标量a &#x3D; torch.tensor(2.5)print(a)pri">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch-Tensor基础">
<meta property="og:url" content="https://kallenblue.github.io/2022/10/19/Pytorch/Pytorch-Tensor%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="Code Play">
<meta property="og:description" content="tensor中的shape和size()反映了tensor的格式，二维的tensor的shape&#x2F;size()对应矩阵中的行和列的大小，这样理解即可 1234567891011121314151617181920212223242526272829303132333435import torch# 创建0维的tensor-&gt;标量a &#x3D; torch.tensor(2.5)print(a)pri">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://z4a.net/images/2023/03/14/A42LU_QASEI_7K4.png">
<meta property="article:published_time" content="2022-10-19T12:17:55.000Z">
<meta property="article:modified_time" content="2023-03-10T11:48:07.121Z">
<meta property="article:author" content="南懿烨曦">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://z4a.net/images/2023/03/14/A42LU_QASEI_7K4.png"><link rel="shortcut icon" href="https://s1.ax1x.com/2023/03/14/pplXlLj.png"><link rel="canonical" href="https://kallenblue.github.io/2022/10/19/Pytorch/Pytorch-Tensor%E5%9F%BA%E7%A1%80/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 南懿烨曦","link":"链接: ","source":"来源: Code Play","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'mediumZoom',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorch-Tensor基础',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-10 19:48:07'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.1"><link rel="alternate" href="/atom.xml" title="Code Play" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s1.ax1x.com/2023/03/13/ppQmllj.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">40</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">16</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://z4a.net/images/2023/03/14/A42LU_QASEI_7K4.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Code Play</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Pytorch-Tensor基础</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-10-19T12:17:55.000Z" title="发表于 2022-10-19 20:17:55">2022-10-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-10T11:48:07.121Z" title="更新于 2023-03-10 19:48:07">2023-03-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Pytorch/">Pytorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Pytorch-Tensor基础"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="tensor中的shape和size"><a href="#tensor中的shape和size" class="headerlink" title="tensor中的shape和size()"></a>tensor中的shape和size()</h1><p>反映了tensor的格式，二维的tensor的shape/size()对应矩阵中的行和列的大小，这样理解即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建0维的tensor-&gt;标量</span></span><br><span class="line">a = torch.tensor(<span class="number">2.5</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a.size())</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># tensor(2.5000)</span></span><br><span class="line"><span class="comment"># torch.Size([])</span></span><br><span class="line"><span class="comment"># torch.Size([])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个tensor</span></span><br><span class="line">a = torch.tensor([<span class="number">2.2</span>, <span class="number">3.5</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a.size())</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([2.2000, 3.5000])</span></span><br><span class="line"><span class="comment"># torch.Size([2])</span></span><br><span class="line"><span class="comment"># torch.Size([2])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个二维度的tensor</span></span><br><span class="line">a = torch.FloatTensor(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a.size())</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([[3.2605e-12, 7.4086e+28, 7.3697e+28],</span></span><br><span class="line"><span class="comment">#         [1.0947e+27, 1.7785e+25, 1.2122e+04],</span></span><br><span class="line"><span class="comment">#         [7.1846e+22, 6.9983e+28, 2.9593e+21]])</span></span><br><span class="line"><span class="comment"># torch.Size([3, 3])</span></span><br><span class="line"><span class="comment"># torch.Size([3, 3])</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="不同维度的tensor"><a href="#不同维度的tensor" class="headerlink" title="不同维度的tensor"></a>不同维度的tensor</h1><h2 id="0，1维"><a href="#0，1维" class="headerlink" title="0，1维"></a>0，1维</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0维</span></span><br><span class="line">a = torch.tensor(<span class="number">2.2</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="comment"># 返回维度</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(a.shape))</span><br><span class="line"><span class="built_in">print</span>(a.size())</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># torch.Size([])</span></span><br><span class="line"><span class="comment"># 0</span></span><br><span class="line"><span class="comment"># torch.Size([])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1维</span></span><br><span class="line">a = torch.tensor([<span class="number">1.1</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([1.1000])</span></span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">1.1</span>, <span class="number">2.2</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([1.1000, 2.2000])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一个一维，长度为1的tensor</span></span><br><span class="line">a = torch.FloatTensor(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([1.4013e-45])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机创建一个一维，长度为2的tensor</span></span><br><span class="line">a = torch.FloatTensor(<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([0.0000, 1.8750])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个一维的，长度为2且元素值为1的向量</span></span><br><span class="line">data = np.ones(<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br><span class="line"><span class="comment"># 根据向量创建tensor</span></span><br><span class="line">a = torch.from_numpy(data)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># [1. 1.]</span></span><br><span class="line"><span class="comment"># tensor([1., 1.], dtype=torch.float64)</span></span><br></pre></td></tr></table></figure>
<h2 id="二、三维"><a href="#二、三维" class="headerlink" title="二、三维"></a>二、三维</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 根据正态分布随机生成tensor</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a.size(<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(a.size(<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(a.shape[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([[-0.2871, -0.2379,  0.0436],</span></span><br><span class="line"><span class="comment">#         [ 1.5324,  0.7144,  0.0508]])</span></span><br><span class="line"><span class="comment"># torch.Size([2, 3])</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据正态分布随机生成</span></span><br><span class="line">a = torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(a.shape))</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([[[0.3967, 0.1736, 0.6457],</span></span><br><span class="line"><span class="comment">#          [0.8819, 0.7055, 0.3417]],</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#         [[0.6029, 0.6032, 0.4393],</span></span><br><span class="line"><span class="comment">#          [0.8388, 0.0709, 0.0857]]])</span></span><br><span class="line"><span class="comment"># torch.Size([2, 2, 3])</span></span><br><span class="line"><span class="comment"># tensor([[0.3967, 0.1736, 0.6457],</span></span><br><span class="line"><span class="comment">#         [0.8819, 0.7055, 0.3417]])</span></span><br><span class="line"><span class="comment"># [2, 2, 3]</span></span><br></pre></td></tr></table></figure>
<h2 id="四维及其他"><a href="#四维及其他" class="headerlink" title="四维及其他"></a>四维及其他</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.rand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="comment"># 返回tensor元素的个数：2*3*4*2</span></span><br><span class="line"><span class="built_in">print</span>(a.numel())</span><br><span class="line"><span class="comment"># 返回tensor元素的维度</span></span><br><span class="line"><span class="built_in">print</span>(a.dim())</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([[[[0.5599, 0.2723],</span></span><br><span class="line"><span class="comment">#           [0.0488, 0.2327],</span></span><br><span class="line"><span class="comment">#           [0.2166, 0.8122],</span></span><br><span class="line"><span class="comment">#           [0.0877, 0.0184]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#          [[0.4803, 0.9102],</span></span><br><span class="line"><span class="comment">#           [0.3802, 0.5224],</span></span><br><span class="line"><span class="comment">#           [0.1912, 0.8837],</span></span><br><span class="line"><span class="comment">#           [0.9155, 0.5591]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#          [[0.3846, 0.7604],</span></span><br><span class="line"><span class="comment">#           [0.7920, 0.7762],</span></span><br><span class="line"><span class="comment">#           [0.7335, 0.5712],</span></span><br><span class="line"><span class="comment">#           [0.2319, 0.5643]]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[[0.3215, 0.3339],</span></span><br><span class="line"><span class="comment">#           [0.4448, 0.2315],</span></span><br><span class="line"><span class="comment">#           [0.7863, 0.2281],</span></span><br><span class="line"><span class="comment">#           [0.1447, 0.7601]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#          [[0.9106, 0.8218],</span></span><br><span class="line"><span class="comment">#           [0.6886, 0.8535],</span></span><br><span class="line"><span class="comment">#           [0.9112, 0.8106],</span></span><br><span class="line"><span class="comment">#           [0.2282, 0.9450]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#          [[0.5409, 0.3910],</span></span><br><span class="line"><span class="comment">#           [0.6941, 0.9943],</span></span><br><span class="line"><span class="comment">#           [0.6982, 0.7778],</span></span><br><span class="line"><span class="comment">#           [0.7190, 0.3657]]]])</span></span><br><span class="line"><span class="comment"># torch.Size([2, 3, 4, 2])</span></span><br><span class="line"><span class="comment"># 48</span></span><br><span class="line"><span class="comment"># 4</span></span><br></pre></td></tr></table></figure>
<h1 id="创建tensor"><a href="#创建tensor" class="headerlink" title="创建tensor"></a>创建tensor</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建一个一维，长度为2的array，再通过torch创建tensor</span></span><br><span class="line">array = np.array([<span class="number">2.1</span>, <span class="number">3.3</span>])</span><br><span class="line">a = torch.from_numpy(array)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([2.1000, 3.3000], dtype=torch.float64)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个二维，长度分别为2和3的array，再通过torch创建tensor</span></span><br><span class="line">array = np.ones([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">a = torch.from_numpy(array)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]], dtype=torch.float64)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch通过list直接创建tensor</span></span><br><span class="line">a = torch.tensor([<span class="number">2.</span>, <span class="number">3.2</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a = torch.tensor([[<span class="number">2.</span>, <span class="number">3.2</span>], [<span class="number">1.</span>, <span class="number">2.6</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([2.0000, 3.2000])</span></span><br><span class="line"><span class="comment"># tensor([[2.0000, 3.2000],</span></span><br><span class="line"><span class="comment">#         [1.0000, 2.6000]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tensor()方法根据参数个数创建对应维度的tensor，每个维度的长度为参数的大小</span></span><br><span class="line">a = torch.Tensor(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 未初始化的tensor</span></span><br><span class="line">a = torch.IntTensor(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line">a = torch.FloatTensor(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([[1, 0, 1],</span></span><br><span class="line"><span class="comment">#         [0, 1, 0]], dtype=torch.int32)</span></span><br><span class="line"><span class="comment"># tensor([[-1.5556e-18,  5.3249e-43,  0.0000e+00],</span></span><br><span class="line"><span class="comment">#         [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])</span></span><br></pre></td></tr></table></figure>
<p>tensor创建默认是FloatTensor类型，可以通过下面的方式修改默认</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.set_default_tensor_type(torch.DoubleTensor)</span><br></pre></td></tr></table></figure>
<h2 id="rand"><a href="#rand" class="headerlink" title="rand*"></a>rand*</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机创建一个值在(0,1)的3*3二维tensor</span></span><br><span class="line">a = torch.rand(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 传入一个tensor并执行rand函数创建新的tensor</span></span><br><span class="line">a = torch.rand_like(a)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 创建一个值在[1, 10)的3*3二维tensor</span></span><br><span class="line">a = torch.randint(<span class="number">1</span>, <span class="number">10</span>, [<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([[0.2667, 0.3021, 0.1402],</span></span><br><span class="line"><span class="comment">#         [0.9188, 0.2889, 0.4163],</span></span><br><span class="line"><span class="comment">#         [0.2108, 0.9262, 0.5140]])</span></span><br><span class="line"><span class="comment"># tensor([[0.6878, 0.0178, 0.3735],</span></span><br><span class="line"><span class="comment">#         [0.8983, 0.6910, 0.3853],</span></span><br><span class="line"><span class="comment">#         [0.1530, 0.5895, 0.7096]])</span></span><br><span class="line"><span class="comment"># tensor([[2, 5, 6],</span></span><br><span class="line"><span class="comment">#         [6, 9, 8],</span></span><br><span class="line"><span class="comment">#         [8, 6, 7]])</span></span><br></pre></td></tr></table></figure>
<h2 id="full"><a href="#full" class="headerlink" title="full"></a>full</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># full,创建一个2*3，且元素值皆为7的tensor</span></span><br><span class="line">a = torch.full([<span class="number">2</span>, <span class="number">3</span>], <span class="number">7</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 创建一个元素为7的标量</span></span><br><span class="line">a = torch.full([], <span class="number">7</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([[7, 7, 7],</span></span><br><span class="line"><span class="comment">#         [7, 7, 7]])</span></span><br><span class="line"><span class="comment"># tensor(7)</span></span><br></pre></td></tr></table></figure>
<h2 id="arange"><a href="#arange" class="headerlink" title="arange"></a>arange</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个差值为1的等差一维tensor</span></span><br><span class="line">a = torch.arange(<span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 创建一个差值为2的等差一维tensor</span></span><br><span class="line">a = torch.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"><span class="comment"># tensor([0, 2, 4, 6, 8])</span></span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:869083577@qq.com">南懿烨曦</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://kallenblue.github.io/2022/10/19/Pytorch/Pytorch-Tensor%E5%9F%BA%E7%A1%80/">https://kallenblue.github.io/2022/10/19/Pytorch/Pytorch-Tensor基础/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://kallenblue.github.io" target="_blank">Code Play</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://z4a.net/images/2023/03/14/A42LU_QASEI_7K4.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/10/20/Java%E9%9D%A2%E8%AF%95/Java%E9%9D%A2%E8%AF%95-%E5%AE%B9%E5%99%A8/"><img class="prev-cover" src="https://s1.ax1x.com/2023/03/14/pplXtYV.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Java面试-容器</div></div></a></div><div class="next-post pull-right"><a href="/2022/10/17/Nginx/Nginx%E5%9F%BA%E7%A1%80/"><img class="next-cover" src="https://s1.ax1x.com/2023/03/14/pplXNWT.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Nginx基础</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Livere</span><span class="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div id="lv-container" data-id="city" data-uid="MTAyMC81NzI0MS8zMzcwNQ=="></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#tensor%E4%B8%AD%E7%9A%84shape%E5%92%8Csize"><span class="toc-number">1.</span> <span class="toc-text">tensor中的shape和size()</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E7%BB%B4%E5%BA%A6%E7%9A%84tensor"><span class="toc-number">2.</span> <span class="toc-text">不同维度的tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0%EF%BC%8C1%E7%BB%B4"><span class="toc-number">2.1.</span> <span class="toc-text">0，1维</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E4%B8%89%E7%BB%B4"><span class="toc-number">2.2.</span> <span class="toc-text">二、三维</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E7%BB%B4%E5%8F%8A%E5%85%B6%E4%BB%96"><span class="toc-number">2.3.</span> <span class="toc-text">四维及其他</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAtensor"><span class="toc-number">3.</span> <span class="toc-text">创建tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#rand"><span class="toc-number">3.1.</span> <span class="toc-text">rand*</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#full"><span class="toc-number">3.2.</span> <span class="toc-text">full</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#arange"><span class="toc-number">3.3.</span> <span class="toc-text">arange</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('/image/footer-bg.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By 南懿烨曦</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">wow~你居然翻到了"页脚"~QWQ</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><div class="js-pjax"><script>function loadLivere () {
  if (typeof LivereTower === 'object') {
    window.LivereTower.init()
  }
  else {
    (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
    })(document, 'script');
  }
}

if ('Livere' === 'Livere' || !false) {
  if (false) btf.loadComment(document.getElementById('lv-container'), loadLivere)
  else loadLivere()
}
else {
  function loadOtherComment () {
    loadLivere()
  }
}</script><script>function loadDisqus () {
  var disqus_config = function () {
    this.page.url = 'https://kallenblue.github.io/2022/10/19/Pytorch/Pytorch-Tensor%E5%9F%BA%E7%A1%80/'
    this.page.identifier = '2022/10/19/Pytorch/Pytorch-Tensor基础/'
    this.page.title = 'Pytorch-Tensor基础'
  };

  window.disqusReset = () => {
    DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  if (window.DISQUS) disqusReset()
  else {
    (function() { 
      var d = document, s = d.createElement('script');
      s.src = 'https://httpskallenbluegithubio.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  }
}

if ('Livere' === 'Disqus' || !false) {
  if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
  else loadDisqus()
} else {
  function loadOtherComment () {
    loadDisqus()
  }
}
</script></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="Hiraeth,Ephemeral,Limerence,Sonorous,Aurora,Epoch,Epiphany,Petrichor" data-fontsize="15px" data-random="true" async="async"></script><script src="//code.tidio.co/lvwsl5yorrlxergsbml1bxzfxmcqmojl.js" async="async"></script><script>function onTidioChatApiReady() {
  window.tidioChatApi.hide();
  window.tidioChatApi.on("close", function() {
    window.tidioChatApi.hide();
  });
}
if (window.tidioChatApi) {
  window.tidioChatApi.on("ready", onTidioChatApiReady);
} else {
  document.addEventListener("tidioChat-ready", onTidioChatApiReady);
}

var chatBtnFn = () => {
  document.getElementById("chat_btn").addEventListener("click", function(){
    window.tidioChatApi.show();
    window.tidioChatApi.open();
  });
}
chatBtnFn()
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>